Threads and life cycle associated with it.
Synchronization, wait and notify keyword.
thread safe data structure and parallelization along with alogith i.e merge sort
stream APi.



Theory behind Multithreading
threads Manipulation
wait-notify
inter thread communication
executors
examples for multithreading



by default, Programming languages are sequential which means they execute command one by one i.e on a line by line basis.

public static void main(String args[])
{
	initializeArrays();
	downloadData();
	buildModel();
	makePredictions();
}

In a Single threaded application, these opeartions are executed one after the another.Time consuming operations may freeze the entire application.so single threaded application is not practical.

Multithreading is the ability of the CPU to execute Multiple processes or threads concurrently.Both threads and processes are independent sequences of execution.


1)Processes : a process is an instance of program execution.
for e.g opening a software (paint,excel etc) or web browser- these are distict processes
The operating system assigns distict registers,stack memory and heap memory to every single process to ecery single Application.

ProcessBuilder class is used to create processes.

cmd : TaskList

PID: Process Identification Number
 
 process is not preferred because we need to create registers and assign memory which is expensive process and processes within a given OS form a directed graph in sense that every single process has so called parent process, so when we create a new process we have to ahandle the parent process as well.


 so we go with Threads

 a thread is a essentially a alight weight process, a thread is a unit of execution within a single process, so a single process can 
 contain several threads.
 Each thread in a process shares the memory and the resources and this is why programmers have to deal with concurrent programming 
 i.e synchronization to make data consistent stoted in memory of process i,e registers, stack and heap which is shared among several threads within a process.



 Time Slicing Algorithm:
 Generally considering only one cpu, i.e single processor several threads gets into execution on by one but 
 on multi core i.e many cpu threads can run parallely no time slicing needed i.e parralelization


 Advantages of Multithreading

 1:we can design more responsive applications - we can do several operations concurrently.
 2: we can achieve better resource utilization i.e CPU utilization. BY default every java application is a single threaded.we can
 utilize more CPU cores with multiple threads.
 3: we can improve performance- we can utilize multiple CPU cores and run the thraeds in parallel

Downsides of MultiThreading

1: threads are manipulating data that are located on the same memory area because they belong to the same process ,synchronization
is not that straight forward.

2:Not easy to design and test multi threaded application

3:Using Multiple threads is expensive - CPU has to save local data, application pointer etc. of the current thread and has to load other thread as well, so context switching is expensive.

with few threads e can decrease processing time but after ceratin number of thread processing time increases, GENERALLY algorithm are 
sequential except few such as Quick and Merge sort are run in parallel with multiple threads,

Thread LifeCycles

1: New state: Every Thread is in new state untill we call the start() method.
2: Active state: Every when we call the start() method on a given thread,there are two substates runnable(ready to be picked by cpu)
and running
3: Blocked /Waiting : when we call join(),sleep(),wait()or thread is waiting for another thread to finish.IN this casew
thread doesnot need cpu cycles  so essentialy it is waiting and threadscheduler will inform them to execute.an ctive can become block and vice cersa for e.g producer and consumer i.e wait() and notify()(NO CPU CYCLES)
4: Terminated state : when the thread has finished its task.


There are two fundamental ways to create thread
1: extend Thread class.
2: Implement Runnable Interface.

There are some other ways as well like executor etc.

The Runnable interface should be implemented by any class whose instances are intended to be executed by the thread.
so the class must define a method of no argument called run.


The Runnable interface in Java is a functional interface introduced in Java 1.0. A functional interface is an interface that 
contains only one abstract method. In the case of Runnable, it has a single abstract method called run().
 This method represents a task that can be executed concurrently.


An anonymous class in Java is a class without a name that's defined and instantiated in a single step. It's typically used when you need to create a class with a small,
 one-time use, and it's not worth defining a separate class for it.
 
 e.g
 
 Thread t2 = new Thread(new Runnable() { // Anonymous class implementing Runnable
    public void run() { // Implementation of the run() method
        for (int i = 0; i < 10; i++) {
            System.out.println("Anonymous 2 " + i);
        }
    }
});



we can use several other methods like sleep(),wait, join etc.
but needs to be handled in exception case

if main has to wait for t1: t1.join()
so main will wait till t1 dies.(uninterrupted exceptions needs to be handled)

daemon Thread and Worker Thread:
daemon thread are low priority thread, which can be created and automatically expires onec worker thread dies.


Thread priority:
Thread scheduler is a part og JVM
we can assign a priority value (1 -10) to every thread
1: MIN_PRIORITY
2: MAX_PRIORITY
default prioty value is 5
Thread with higher priority will be executed first

if all are of same priority , suppose 5, FCFS which is stored in Queue abstract data type.

but marking thread as high priority just increases its probabilty but dose not ensure it will work first,because 
it depends on  the underlying oS (thread starvation is avoided)


intrinsic lock  or Monitor lock of object or class

private static final object lock1 = new object();
private static final object lock2 = new object();

//at the same time != parallelization - CPU Time slicing

//re entrant lock
In the previous lectures we have been talking about locks (intrinsic locks or monitor locks).

there is a single intrinsic lock associated with every object or class in Java

a given  thread that needs exclusive and consistent access to an object's fields

has to acquire the object's intrinsic lock before accessing them,

and then the thread releases the intrinsic lock when it's done with them

with Locks: the acquired lock can be released any thread

RLocks can be released by the thread that acquired it exclusively

Ok so a thread cannot acquire a lock owned by another thread. But a given thread can acquire a lock that 
it already owns. Allowing a thread to acquire the same lock more than once is called re-entrant synchronization.
 And this is exactly what is happening in Python when using RLocks- the same thread may acquire the lock more than once.

For example: let's consider recursive method calls. If a given thread calls a recursive and synchronized method
 several times then it is fine (note that in this case the same thread "enters" the synchronized block several times).
 There will be no deadlock because of re-entrant synchronization.


Threads Communication:
Threads that are locking the same intrinsic lock (monitor) can relese the lock until the other thread calls notify.
these wait and notify methods can be used and called from sunchronized method or blocks exclusively.


synchronized block me notify ke bad bhi remaining code chalta hai, phir bad me lock ko deta hai.

you call wait on the Object while on the other hand you call sleep on the Thread itself

wait can be interrupter (this is why we need the InterruptedException) while on the other hand sleep can not

wait (and notify) must happen in a synchronized  block on the monitor object whereas sleep does not

sleep operation does not release the locks it holds while on the other hand wait releases the lock on the object
 that wait() is called on

notify  is not going to notify immediately,it will further do the operation of the block(synchronized);

 A reentrant lock has same basic behaviour as we have seen for synchronized blocks(with some extended features)
 
 we can make  a lock fair: prevent thread starvation 
 Synchronized blocks are unfair by default.
 
 we can check whether the given lock is held or not with reentrant locks.
 
 we can get the list of threads waiting for the given lock with reentrant locks
  synchronized blocks are nicer: we do not need the try-catch-finally block.
  
  Heap Memmory is slower but large region in the RAM where the objects(refrence types) are being stored
  
  
 |-------|----------------|      |-------------|      
 | cache |cpu1   thread1  |      |             |
 |-------|----------------|		 |	 Main      |
								 | 	Memory     |
								 |	           |
								 | 			   |	
 |-------|----------------|		 |-------------|	
 | cache |cpu2   thread2  |
 |-------|----------------|


Every single thread has its own stack memory,own central processing unit and its own cache

Every read of a voltaile variable will be read from the RAM so from the main memory ( and not from cache )
usually variable are cached for performance reasons
caches are faster.Do not use volatile keyword if not necessary.
(+ it prevents instructions reordering which is a performance boost technique)


Deadlock in database and operating system.
How to handle daeadlock and livelock ?
TRy to avoid cyclic dependency.


Atomic INteger


SEMAPHORE: library front desk
binary and counting

semaphore tracks only how many resources are freeze
semaphore count may serve as a useful trigger for a number of differnt action (web servers) medium articles
producer consumer problem


MUTEX(Mutual Exclusion Object)
It is used to prevent race condition i.e several independent thread manipulating the same resource.
mutex is similar to binary semaphore
a lock is designed to enforce a mutual exclusion


Synchronized (this)
synchronized (className.class)--> static ()

SEMAPHORE:

it is a signalling mechanism
threads and processes perform wait() and notify() operations to indicate whether they are acquiring or releasing
the resource
whereas as Mutex is a locking mechanism
thread and processes have to acquire the lock on mutex object if it wants to acquire the resource.


semaphore allows multiple program threads to access the finite instance of resource(not just a single resource)
wheraes mutex allows a multiple program threads to access a single shared resource but one at a time.


the process or threads blocls itself if no resource is  free till the count of semaphore becomes grater than o
if the lock is already acquired by another thread or process then the thread will wait untill the mutex object gets locked.

In the previous lectures we have been talking about binary semaphores and muteness. We have pointed out the fact that there are subtle differences between them. Let's consider the most critical difference again, the so-called principle of ownership.

"Ownership is the simple concept that when a task locks (acquires) a mutex only it can unlock (release) it"

a mutex can be owned by at most one thread at any given time while on the other hand binary semaphore has no concept of ownership

if a task tries to unlock a mutex it hasn’t locked (thus doesn’t own) then an error condition is encountered and, most importantly, the mutex is not unlocked. If the mutual exclusion object doesn’t have ownership then, irrelevant of what it is called, it is not a mutex.

I just wanted to point out the crucial difference between binary

does volatile mean cpu will not cache that variable ?


The volatile keyword in Java serves two main purposes:

Visibility: When a variable is declared as volatile, changes to that variable made by one thread are immediately visible to other threads. This ensures that the most up-to-date value of the variable is always observed by all threads, even if different threads are running on different CPU cores.
Atomicity of Reads and Writes: Although volatile does not provide atomicity for compound actions (such as incrementing a variable), it does ensure that each individual read or write operation on the variable is atomic. In other words, when a thread reads a volatile variable, it sees either the value before or after any write operation, but never a partially updated value.
Now, regarding your question about CPU caching: While the volatile keyword ensures visibility of changes across threads, it doesn't directly control CPU caching behavior. However, in practice, many implementations of Java's volatile semantics involve strategies that minimize or eliminate the use of CPU caches for volatile variables.

When a variable is marked as volatile, the JVM may implement specific memory synchronization mechanisms to ensure that the variable's value is always read from and written to main memory, bypassing CPU caches. This helps maintain visibility of changes across threads and prevents the inconsistency that might arise from different threads caching stale values of the variable.

So, while volatile doesn't guarantee that CPU caching won't be used, its semantics typically lead to behaviors that effectively prevent CPU caching of volatile variables in multi-threaded environments.
lock.tryLock kya hai ?

The `tryLock()` method is a part of the `Lock` interface in Java, provided by classes like `ReentrantLock`. It's used to attempt to acquire the lock without blocking the current thread. Here's an explanation of how it works:

1. **Acquiring the Lock**:
   - When a thread calls `tryLock()` on a `Lock` object, it tries to acquire the lock.
   - If the lock is available (i.e., not held by any other thread), the thread successfully acquires the lock and returns `true`.
   - If the lock is currently held by another thread, the `tryLock()` method immediately returns `false` without waiting. This means the thread does not block and continues execution without acquiring the lock.

2. **Non-Blocking Behavior**:
   - Unlike the `lock()` method, which blocks until the lock is acquired, `tryLock()` is non-blocking. It allows the thread to continue its execution even if it fails to acquire the lock.
   - This non-blocking behavior is useful in scenarios where waiting for the lock indefinitely is not desirable. For example, in situations where a thread needs to perform alternative actions if it cannot acquire the lock immediately.

3. **Handling the Return Value**:
   - After calling `tryLock()`, the thread should check the return value to determine whether it successfully acquired the lock.
   - If `tryLock()` returns `true`, the thread has successfully acquired the lock and can proceed with its critical section of code.
   - If `tryLock()` returns `false`, the thread did not acquire the lock and should execute an alternative path or retry later, depending on the application's logic.

4. **Release of the Lock**:
   - If the thread successfully acquires the lock using `tryLock()`, it's responsible for releasing the lock using the `unlock()` method when it's done with its critical section of code.
   - It's essential to release the lock properly to allow other threads to acquire it.

In summary, `tryLock()` provides a non-blocking mechanism for attempting to acquire a lock. It allows threads to proceed with their execution even if the lock is not immediately available, enabling more flexible control over concurrency in Java programs.































